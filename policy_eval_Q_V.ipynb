{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqwXvWTLNJFL7JIMH+jv7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariihmp/RL_Notebooks/blob/main/policy_eval_Q_V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDSfzxndwsGO",
        "outputId": "c517ea58-28f6-4ecc-e073-cf999afc205c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "project_folder_path = '/content/drive/MyDrive/session-2'\n",
        "sys.path.append(project_folder_path)\n",
        "from GridWorld_env import GridWorld"
      ],
      "metadata": {
        "id": "ys810AI1_t-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GridWorld_env import GridWorld"
      ],
      "metadata": {
        "id": "ynhPk34rDNRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld()\n",
        "def get_random_policy(num_states, num_actions):\n",
        "    return np.ones((num_states, num_actions)) / num_actions\n",
        "\n",
        "random_policy = get_random_policy(env.num_states, env.num_actions)\n",
        "print(\"Random Policy (P(a|s)):\")\n",
        "print(random_policy)"
      ],
      "metadata": {
        "id": "Kr8Q-hpLw4km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94345562-34da-456e-f1d3-bfb6abcab2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Policy (P(a|s)):\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_v(policy, env, theta=1e-6):\n",
        "    V = np.zeros(env.num_states)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s_idx in range(env.num_states):\n",
        "            if env.idx_to_state[s_idx] == env.goal_state: # Value of goal state is 0\n",
        "                continue\n",
        "            v_old = V[s_idx]\n",
        "            new_v = 0\n",
        "            for a_idx in range(env.num_actions):\n",
        "                prob_action = policy[s_idx, a_idx]\n",
        "                next_s_idx, reward, done = env.step(s_idx, a_idx)\n",
        "                new_v += prob_action * (reward + env.gamma * V[next_s_idx])\n",
        "            V[s_idx] = new_v\n",
        "            delta = max(delta, abs(v_old - V[s_idx]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiJWizvZxnZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_q_from_v(V, env):\n",
        "    Q = np.zeros((env.num_states, env.num_actions))\n",
        "    for s_idx in range(env.num_states):\n",
        "        if env.idx_to_state[s_idx] == env.goal_state:\n",
        "            continue\n",
        "        for a_idx in range(env.num_actions):\n",
        "            next_s_idx, reward, done = env.step(s_idx, a_idx)\n",
        "            Q[s_idx, a_idx] = reward + env.gamma * V[next_s_idx]\n",
        "    return Q\n",
        "\n",
        "print(\"\\n--- Evaluating Random Policy ---\")\n",
        "V_random = policy_evaluation_v(random_policy, env)\n",
        "Q_random = calculate_q_from_v(V_random, env)\n",
        "\n",
        "print(\"V(s) for Random Policy:\")\n",
        "for s_idx, v_val in enumerate(V_random):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "print(\"\\nQ(s,a) for Random Policy:\")\n",
        "for s_idx in range(env.num_states):\n",
        "    if env.idx_to_state[s_idx] == env.goal_state:\n",
        "        print(f\"  State {env.idx_to_state[s_idx]} (Goal): Q values are N/A (or 0)\")\n",
        "        continue\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}:\")\n",
        "    for a_idx, q_val in enumerate(Q_random[s_idx]):\n",
        "        print(f\"    Action {env.actions[a_idx]}: {q_val:.2f}\")"
      ],
      "metadata": {
        "id": "RMZv2OrHxv0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9151e254-1533-4bbe-9294-8769ab51954b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Random Policy ---\n",
            "V(s) for Random Policy:\n",
            "  State (0, 0): 1.18\n",
            "  State (0, 1): 3.66\n",
            "  State (1, 0): 3.66\n",
            "  State (1, 1): 0.00\n",
            "\n",
            "Q(s,a) for Random Policy:\n",
            "  State (0, 0):\n",
            "    Action U: 0.06\n",
            "    Action D: 2.30\n",
            "    Action L: 0.06\n",
            "    Action R: 2.30\n",
            "  State (0, 1):\n",
            "    Action U: 2.30\n",
            "    Action D: 10.00\n",
            "    Action L: 0.06\n",
            "    Action R: 2.30\n",
            "  State (1, 0):\n",
            "    Action U: 0.06\n",
            "    Action D: 2.30\n",
            "    Action L: 2.30\n",
            "    Action R: 10.00\n",
            "  State (1, 1) (Goal): Q values are N/A (or 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generalized policy improvement\n",
        "def policy_improvement(V, env):\n",
        "    new_policy = np.zeros((env.num_states, env.num_actions))\n",
        "    for s_idx in range(env.num_states):\n",
        "        if env.idx_to_state[s_idx] == env.goal_state:\n",
        "            # For the goal state, any action is fine or no action is taken\n",
        "            # For simplicity, we can make it uniform or point to a single action.\n",
        "            # However, it doesn't affect the values of other states.\n",
        "            # Let's make it uniform to avoid issues if it's somehow entered as a non-terminal.\n",
        "            new_policy[s_idx, :] = 1.0 / env.num_actions\n",
        "            continue\n",
        "\n",
        "        q_values_s = np.zeros(env.num_actions)\n",
        "        for a_idx in range(env.num_actions):\n",
        "            next_s_idx, reward, done = env.step(s_idx, a_idx)\n",
        "            q_values_s[a_idx] = reward + env.gamma * V[next_s_idx]\n",
        "\n",
        "        best_action_idx = np.argmax(q_values_s)\n",
        "        new_policy[s_idx, best_action_idx] = 1.0 # Greedy action\n",
        "    return new_policy"
      ],
      "metadata": {
        "id": "Ea4ovdk9x3Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, initial_policy=None, theta=1e-6):\n",
        "    if initial_policy is None:\n",
        "        policy = get_random_policy(env.num_states, env.num_actions)\n",
        "    else:\n",
        "        policy = initial_policy.copy()\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        print(f\"\\n--- Policy Iteration: Iteration {iteration} ---\")\n",
        "        # 1. Policy Evaluation\n",
        "        V = policy_evaluation_v(policy, env, theta)\n",
        "        print(\"  V(s) evaluated:\")\n",
        "        for s_idx, v_val in enumerate(V):\n",
        "            print(f\"    State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "        # 2. Policy Improvement\n",
        "        new_policy = policy_improvement(V, env)\n",
        "\n",
        "        print(\"  New Policy (Greedy):\")\n",
        "        for s_idx in range(env.num_states):\n",
        "            print(f\"    State {env.idx_to_state[s_idx]}: Best Action(s) index(es) = {np.where(new_policy[s_idx] == 1)[0]}\")\n",
        "\n",
        "\n",
        "        if np.array_equal(new_policy, policy):\n",
        "            print(\"\\nPolicy converged!\")\n",
        "            break\n",
        "        policy = new_policy\n",
        "\n",
        "    # Final Q values for the optimal policy\n",
        "    Q_optimal = calculate_q_from_v(V, env)\n",
        "    return policy, V, Q_optimal"
      ],
      "metadata": {
        "id": "6Kv7WGhpzt4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nStarting Policy Iteration\")\n",
        "optimal_policy_pi, V_optimal_pi, Q_optimal_pi = policy_iteration(env, initial_policy=random_policy.copy())\n",
        "\n",
        "print(\"\\n Results from Policy Iteration\")\n",
        "print(\"Optimal Policy (PI):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    best_actions = [env.actions[i] for i, p in enumerate(optimal_policy_pi[s_idx]) if p == 1.0]\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: Optimal Action(s) = {best_actions}\")\n",
        "\n",
        "print(\"\\nOptimal V(s) (PI):\")\n",
        "for s_idx, v_val in enumerate(V_optimal_pi):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "print(\"\\nOptimal Q(s,a) (PI):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    if env.idx_to_state[s_idx] == env.goal_state: continue\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}:\")\n",
        "    for a_idx, q_val in enumerate(Q_optimal_pi[s_idx]):\n",
        "        print(f\"    Action {env.actions[a_idx]}: {q_val:.2f}\")"
      ],
      "metadata": {
        "id": "p0yngxSez5cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3759c4ba-ec96-42d5-dc14-5766207cf2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Starting Policy Iteration ---\n",
            "\n",
            "--- Policy Iteration: Iteration 1 ---\n",
            "  V(s) evaluated:\n",
            "    State (0, 0): 1.18\n",
            "    State (0, 1): 3.66\n",
            "    State (1, 0): 3.66\n",
            "    State (1, 1): 0.00\n",
            "  New Policy (Greedy):\n",
            "    State (0, 0): Best Action(s) index(es) = [1]\n",
            "    State (0, 1): Best Action(s) index(es) = [1]\n",
            "    State (1, 0): Best Action(s) index(es) = [3]\n",
            "    State (1, 1): Best Action(s) index(es) = []\n",
            "\n",
            "--- Policy Iteration: Iteration 2 ---\n",
            "  V(s) evaluated:\n",
            "    State (0, 0): 8.00\n",
            "    State (0, 1): 10.00\n",
            "    State (1, 0): 10.00\n",
            "    State (1, 1): 0.00\n",
            "  New Policy (Greedy):\n",
            "    State (0, 0): Best Action(s) index(es) = [1]\n",
            "    State (0, 1): Best Action(s) index(es) = [1]\n",
            "    State (1, 0): Best Action(s) index(es) = [3]\n",
            "    State (1, 1): Best Action(s) index(es) = []\n",
            "\n",
            "Policy converged!\n",
            "\n",
            "--- Results from Policy Iteration ---\n",
            "Optimal Policy (PI):\n",
            "  State (0, 0): Optimal Action(s) = ['D']\n",
            "  State (0, 1): Optimal Action(s) = ['D']\n",
            "  State (1, 0): Optimal Action(s) = ['R']\n",
            "  State (1, 1): Optimal Action(s) = []\n",
            "\n",
            "Optimal V(s) (PI):\n",
            "  State (0, 0): 8.00\n",
            "  State (0, 1): 10.00\n",
            "  State (1, 0): 10.00\n",
            "  State (1, 1): 0.00\n",
            "\n",
            "Optimal Q(s,a) (PI):\n",
            "  State (0, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (0, 1):\n",
            "    Action U: 8.00\n",
            "    Action D: 10.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (1, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 8.00\n",
            "    Action R: 10.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, theta=1e-6):\n",
        "    V = np.zeros(env.num_states)\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        delta = 0\n",
        "        print(f\"\\n--- Value Iteration: Iteration {iteration} ---\")\n",
        "        for s_idx in range(env.num_states):\n",
        "            if env.idx_to_state[s_idx] == env.goal_state:\n",
        "                continue\n",
        "            v_old = V[s_idx]\n",
        "            action_values = np.zeros(env.num_actions)\n",
        "            for a_idx in range(env.num_actions):\n",
        "                next_s_idx, reward, done = env.step(s_idx, a_idx)\n",
        "                action_values[a_idx] = reward + env.gamma * V[next_s_idx]\n",
        "            V[s_idx] = np.max(action_values)\n",
        "            delta = max(delta, abs(v_old - V[s_idx]))\n",
        "\n",
        "        print(\"  V(s) updated:\")\n",
        "        for s_idx_print, v_val_print in enumerate(V):\n",
        "             print(f\"    State {env.idx_to_state[s_idx_print]}: {v_val_print:.2f}\")\n",
        "\n",
        "        if delta < theta:\n",
        "            print(\"\\nValue function converged!\")\n",
        "            break\n",
        "\n",
        "    # Extract optimal policy\n",
        "    optimal_policy = np.zeros((env.num_states, env.num_actions))\n",
        "    for s_idx in range(env.num_states):\n",
        "        if env.idx_to_state[s_idx] == env.goal_state:\n",
        "            optimal_policy[s_idx, :] = 1.0 / env.num_actions # Or any action\n",
        "            continue\n",
        "        action_values = np.zeros(env.num_actions)\n",
        "        for a_idx in range(env.num_actions):\n",
        "            next_s_idx, reward, done = env.step(s_idx, a_idx)\n",
        "            action_values[a_idx] = reward + env.gamma * V[next_s_idx]\n",
        "        best_action_idx = np.argmax(action_values)\n",
        "        optimal_policy[s_idx, best_action_idx] = 1.0\n",
        "\n",
        "    Q_optimal = calculate_q_from_v(V, env) # Or calculate during the last V iteration\n",
        "    return optimal_policy, V, Q_optimal\n"
      ],
      "metadata": {
        "id": "zRmPuh6Sz-hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n--- Starting Value Iteration ---\")\n",
        "optimal_policy_vi, V_optimal_vi, Q_optimal_vi = value_iteration(env)\n",
        "\n",
        "print(\"\\n--- Results from Value Iteration ---\")\n",
        "print(\"Optimal Policy (VI):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    best_actions = [env.actions[i] for i, p in enumerate(optimal_policy_vi[s_idx]) if p == 1.0]\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: Optimal Action(s) = {best_actions}\")\n",
        "\n",
        "print(\"\\nOptimal V(s) (VI):\")\n",
        "for s_idx, v_val in enumerate(V_optimal_vi):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "print(\"\\nOptimal Q(s,a) (VI):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    if env.idx_to_state[s_idx] == env.goal_state: continue\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}:\")\n",
        "    for a_idx, q_val in enumerate(Q_optimal_vi[s_idx]):\n",
        "        print(f\"    Action {env.actions[a_idx]}: {q_val:.2f}\")"
      ],
      "metadata": {
        "id": "DKxgV4kJ0Ghq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b088e60-2252-416e-bf9b-29f37f62d57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Starting Value Iteration ---\n",
            "\n",
            "--- Value Iteration: Iteration 1 ---\n",
            "  V(s) updated:\n",
            "    State (0, 0): -1.00\n",
            "    State (0, 1): 10.00\n",
            "    State (1, 0): 10.00\n",
            "    State (1, 1): 0.00\n",
            "\n",
            "--- Value Iteration: Iteration 2 ---\n",
            "  V(s) updated:\n",
            "    State (0, 0): 8.00\n",
            "    State (0, 1): 10.00\n",
            "    State (1, 0): 10.00\n",
            "    State (1, 1): 0.00\n",
            "\n",
            "--- Value Iteration: Iteration 3 ---\n",
            "  V(s) updated:\n",
            "    State (0, 0): 8.00\n",
            "    State (0, 1): 10.00\n",
            "    State (1, 0): 10.00\n",
            "    State (1, 1): 0.00\n",
            "\n",
            "Value function converged!\n",
            "\n",
            "--- Results from Value Iteration ---\n",
            "Optimal Policy (VI):\n",
            "  State (0, 0): Optimal Action(s) = ['D']\n",
            "  State (0, 1): Optimal Action(s) = ['D']\n",
            "  State (1, 0): Optimal Action(s) = ['R']\n",
            "  State (1, 1): Optimal Action(s) = []\n",
            "\n",
            "Optimal V(s) (VI):\n",
            "  State (0, 0): 8.00\n",
            "  State (0, 1): 10.00\n",
            "  State (1, 0): 10.00\n",
            "  State (1, 1): 0.00\n",
            "\n",
            "Optimal Q(s,a) (VI):\n",
            "  State (0, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (0, 1):\n",
            "    Action U: 8.00\n",
            "    Action D: 10.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (1, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 8.00\n",
            "    Action R: 10.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fEqLnFdVBShs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}