{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkP5YImvGWSSEFS28tNNpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariihmp/RL_Notebooks/blob/main/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lVtVSzUDwI9",
        "outputId": "25411b32-200f-4db3-bc0c-669bc9164a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "project_folder_path = '/content/drive/MyDrive/session-3'\n",
        "sys.path.append(project_folder_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "zPl8c9BkD3_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GridWorld_env import GridWorld\n",
        "env = GridWorld()"
      ],
      "metadata": {
        "id": "UrGTn79nJYpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfUnS5Vs4k0l"
      },
      "outputs": [],
      "source": [
        "\n",
        "def q_learning(env, num_episodes, alpha=0.1, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_rate=0.999):\n",
        "    Q = np.zeros((env.num_states, env.num_actions))\n",
        "\n",
        "    goal_s_idx = env.state_to_idx[env.goal_state]\n",
        "    Q[goal_s_idx, :] = 0\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    print(f\"\\nStarting Q-learning for {num_episodes} episodes\")\n",
        "    print(f\"Alpha={alpha}, Epsilon_start={epsilon_start}, Epsilon_end={epsilon_end}, Epsilon_decay={epsilon_decay_rate}\")\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        if (i_episode + 1) % (num_episodes // 20 if num_episodes >=20 else 1) == 0:\n",
        "            current_v_for_start = np.max(Q[env.state_to_idx[env.start_state]]) if env.state_to_idx[env.start_state] < Q.shape[0] else 0\n",
        "            print(f\"Episode {i_episode + 1}/{num_episodes}, Epsilon: {epsilon:.3f}, Q(start, max_a): {current_v_for_start:.2f}\")\n",
        "\n",
        "\n",
        "        state_idx = env.reset()\n",
        "        done = False\n",
        "        max_steps_per_episode = 100\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps_per_episode:\n",
        "            if env.idx_to_state[state_idx] == env.goal_state:\n",
        "                break\n",
        "\n",
        "            # epsilon-greedy action selection\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action_idx = random.choice(range(env.num_actions))\n",
        "            else:\n",
        "                action_idx = np.argmax(Q[state_idx])\n",
        "                best_actions = np.flatnonzero(Q[state_idx] == np.max(Q[state_idx]))\n",
        "                action_idx = np.random.choice(best_actions)\n",
        "\n",
        "\n",
        "            next_state_idx, reward, done = env.step(state_idx, action_idx)\n",
        "\n",
        "            # Q-learning update\n",
        "            if done:\n",
        "                td_target = reward\n",
        "                Q[state_idx, action_idx] += alpha * (td_target - Q[state_idx, action_idx])\n",
        "            else:\n",
        "                best_next_action_q = np.max(Q[next_state_idx])\n",
        "                td_target = reward + env.gamma * best_next_action_q\n",
        "                Q[state_idx, action_idx] += alpha * (td_target - Q[state_idx, action_idx])\n",
        "\n",
        "            Q[goal_s_idx, :] = 0\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "            steps += 1\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay_rate)\n",
        "\n",
        "\n",
        "    # Extract optimal policy from Q*\n",
        "    optimal_policy_qlearning = np.zeros((env.num_states, env.num_actions))\n",
        "    for s_idx in range(env.num_states):\n",
        "        if env.idx_to_state[s_idx] == env.goal_state:\n",
        "            optimal_policy_qlearning[s_idx, :] = 1.0 / env.num_actions\n",
        "            continue\n",
        "        best_action = np.argmax(Q[s_idx])\n",
        "        optimal_policy_qlearning[s_idx, best_action] = 1.0\n",
        "\n",
        "    V_optimal_qlearning = np.max(Q, axis=1)\n",
        "    V_optimal_qlearning[goal_s_idx] = 0\n",
        "\n",
        "    return optimal_policy_qlearning, V_optimal_qlearning, Q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_q_episodes = 5000\n",
        "alpha_q = 0.1 # Learning rate\n",
        "\n",
        "#ep parmas\n",
        "epsilon_start_q = 1.0\n",
        "epsilon_end_q = 0.05\n",
        "epsilon_decay_q = 0.999 # Decays epsilon over episodes\n",
        "\n",
        "\n",
        "optimal_policy_q, V_optimal_q, Q_star_q = q_learning(env,\n",
        "                                                      num_q_episodes,\n",
        "                                                      alpha=alpha_q,\n",
        "                                                      epsilon_start=epsilon_start_q,\n",
        "                                                      epsilon_end=epsilon_end_q,\n",
        "                                                      epsilon_decay_rate=epsilon_decay_q)\n",
        "\n",
        "print(\"\\nResults from Q-learning\")\n",
        "print(\"Optimal Policy (Q-learning):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    best_actions_q = [env.actions[i] for i, p in enumerate(optimal_policy_q[s_idx]) if p == 1.0]\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: Optimal Action(s) = {best_actions_q}\")\n",
        "\n",
        "print(\"\\nOptimal V(s) (Q-learning, from Q*):\")\n",
        "for s_idx, v_val in enumerate(V_optimal_q):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "print(\"\\nOptimal Q*(s,a) (Q-learning):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    if env.idx_to_state[s_idx] == env.goal_state:\n",
        "        print(f\"  State {env.idx_to_state[s_idx]} (Goal): Q values are 0.00\")\n",
        "        continue\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}:\")\n",
        "    for a_idx, q_val in enumerate(Q_star_q[s_idx]):\n",
        "        print(f\"    Action {env.actions[a_idx]}: {q_val:.2f}\")\n",
        "\n",
        "print(\"\\nFor comparison, Q*(s,a) from Value Iteration (should be similar):\")\n",
        "for s_idx in range(env.num_states):\n",
        "    if env.idx_to_state[s_idx] == env.goal_state:\n",
        "        print(f\"  State {env.idx_to_state[s_idx]} (Goal): Q values are 0.00\")\n",
        "        continue\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}:\")\n",
        "\n",
        "    #import Q_optimal_vi\n",
        "    # for a_idx, q_val in enumerate(Q_optimal_vi[s_idx]):\n",
        "    #     print(f\"    Action {env.actions[a_idx]}: {q_val:.2f}\")"
      ],
      "metadata": {
        "id": "87eo8aBC6DeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11146f11-14b8-4915-d242-65a8dd31d951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Q-learning for 5000 episodes\n",
            "Alpha=0.1, Epsilon_start=1.0, Epsilon_end=0.05, Epsilon_decay=0.999\n",
            "Episode 250/5000, Epsilon: 0.779, Q(start, max_a): 8.00\n",
            "Episode 500/5000, Epsilon: 0.607, Q(start, max_a): 8.00\n",
            "Episode 750/5000, Epsilon: 0.473, Q(start, max_a): 8.00\n",
            "Episode 1000/5000, Epsilon: 0.368, Q(start, max_a): 8.00\n",
            "Episode 1250/5000, Epsilon: 0.287, Q(start, max_a): 8.00\n",
            "Episode 1500/5000, Epsilon: 0.223, Q(start, max_a): 8.00\n",
            "Episode 1750/5000, Epsilon: 0.174, Q(start, max_a): 8.00\n",
            "Episode 2000/5000, Epsilon: 0.135, Q(start, max_a): 8.00\n",
            "Episode 2250/5000, Epsilon: 0.105, Q(start, max_a): 8.00\n",
            "Episode 2500/5000, Epsilon: 0.082, Q(start, max_a): 8.00\n",
            "Episode 2750/5000, Epsilon: 0.064, Q(start, max_a): 8.00\n",
            "Episode 3000/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 3250/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 3500/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 3750/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 4000/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 4250/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 4500/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 4750/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "Episode 5000/5000, Epsilon: 0.050, Q(start, max_a): 8.00\n",
            "\n",
            "Results from Q-learning\n",
            "Optimal Policy (Q-learning):\n",
            "  State (0, 0): Optimal Action(s) = ['D']\n",
            "  State (0, 1): Optimal Action(s) = ['D']\n",
            "  State (1, 0): Optimal Action(s) = ['R']\n",
            "  State (1, 1): Optimal Action(s) = []\n",
            "\n",
            "Optimal V(s) (Q-learning, from Q*):\n",
            "  State (0, 0): 8.00\n",
            "  State (0, 1): 10.00\n",
            "  State (1, 0): 10.00\n",
            "  State (1, 1): 0.00\n",
            "\n",
            "Optimal Q*(s,a) (Q-learning):\n",
            "  State (0, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (0, 1):\n",
            "    Action U: 8.00\n",
            "    Action D: 10.00\n",
            "    Action L: 6.20\n",
            "    Action R: 8.00\n",
            "  State (1, 0):\n",
            "    Action U: 6.20\n",
            "    Action D: 8.00\n",
            "    Action L: 8.00\n",
            "    Action R: 10.00\n",
            "  State (1, 1) (Goal): Q values are 0.00\n",
            "\n",
            "For comparison, Q*(s,a) from Value Iteration (should be similar):\n",
            "  State (0, 0):\n",
            "  State (0, 1):\n",
            "  State (1, 0):\n",
            "  State (1, 1) (Goal): Q values are 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--zf3m9rEXcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}