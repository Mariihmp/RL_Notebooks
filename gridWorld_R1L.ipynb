{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3knGOaYzA3MH8b49V10et",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariihmp/RL_Notebooks/blob/main/gridWorld_R1L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "up = 0\n",
        "down = 1\n",
        "left = 2\n",
        "right = 3\n",
        "\n",
        "# no. of states and no. of variables\n",
        "grid_size = 5\n",
        "noS = grid_size * grid_size\n",
        "noA = 4\n",
        "\n",
        "S = range(noS)\n",
        "\n",
        "reward = -1  # for every step\n",
        "\n",
        "terminal_state = lambda s: s == 0 or s == noS - 1  # state terminal\n",
        "wall = []\n",
        "\n",
        "P = dict()\n",
        "\n",
        "for s in S:\n",
        "    P[s] = dict()\n",
        "\n",
        "    if (terminal_state(s)):\n",
        "        P[s][up] = (s, 1.0, 0.0)\n",
        "        P[s][down] = (s, 1.0, 0.0)\n",
        "        P[s][right] = (s, 1.0, 0.0)\n",
        "        P[s][left] = (s, 1.0, 0.0)\n",
        "    else:\n",
        "        # Up\n",
        "        next_s_up = s - grid_size\n",
        "        if next_s_up < 0:  # top boundary\n",
        "            P[s][up] = (s, 1.0, -1000.0)\n",
        "        elif next_s_up in wall:\n",
        "            P[s][up] = (s, 1.0, -1000.0)\n",
        "        else:\n",
        "            P[s][up] = (next_s_up, 1.0, reward)\n",
        "\n",
        "        # Down\n",
        "        next_s_down = s + grid_size\n",
        "        if next_s_down >= noS:  # bottom boundary\n",
        "            P[s][down] = (s, 1.0, -1000.0)\n",
        "        elif next_s_down in wall:\n",
        "            P[s][down] = (s, 1.0, -1000.0)\n",
        "        else:\n",
        "            P[s][down] = (next_s_down, 1.0, reward)\n",
        "\n",
        "        # Right\n",
        "        next_s_right = s + 1\n",
        "        if (s + 1) % grid_size == 0:  #  right boundary\n",
        "            P[s][right] = (s, 1.0, -1000.0)\n",
        "        elif next_s_right in wall:\n",
        "            P[s][right] = (s, 1.0, -1000.0)\n",
        "        else:\n",
        "            P[s][right] = (next_s_right, 1.0, reward)\n",
        "\n",
        "        # Left\n",
        "        next_s_left = s - 1\n",
        "        if s % grid_size == 0:  # left boundary\n",
        "            P[s][left] = (s, 1.0, -1000.0)\n",
        "        elif next_s_left in wall:\n",
        "            P[s][left] = (s, 1.0, -1000.0)\n",
        "        else:\n",
        "            P[s][left] = (next_s_left, 1.0, reward)\n",
        "\n",
        "\n",
        "print('No. of states in grid: ', noS)\n",
        "print('No. of action options in each state:', noA)\n",
        "Action_Index = dict()\n",
        "Action_Index[0] = 'up'\n",
        "Action_Index[1] = 'down'\n",
        "Action_Index[2] = 'left'\n",
        "Action_Index[3] = 'right'\n",
        "Action_Index[5] = 'terminal states (stay)'\n",
        "Action_Index[7] = 'wall'\n",
        "\n",
        "print('Index for actions:')\n",
        "for k, v in Action_Index.items():\n",
        "    print(k, \":\", v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgUp-bwYLjS",
        "outputId": "82ced5ab-efa3-402d-8a55-1af7231e102f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of states in grid:  25\n",
            "No. of action options in each state: 4\n",
            "Index for actions:\n",
            "0 : up\n",
            "1 : down\n",
            "2 : left\n",
            "3 : right\n",
            "5 : terminal states (stay)\n",
            "7 : wall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The value fuction Vpi for a given policy pi (policy evaluation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XgbJuLvvbHDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(P, policy, threshold, discount):\n",
        "    value = np.zeros((noS,))\n",
        "    while True:\n",
        "        new_value = np.zeros((noS,))\n",
        "\n",
        "        change = 0\n",
        "        for s in S:\n",
        "            v = 0\n",
        "\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                next_state, probability, reward = P[s][a]\n",
        "                temp = probability * action_prob * (reward + discount * value[next_state])\n",
        "                v += temp\n",
        "\n",
        "            change = max(change, np.abs(v - value[s]))\n",
        "            new_value[s] = v\n",
        "\n",
        "        if change < threshold:\n",
        "            break\n",
        "\n",
        "        value = new_value\n",
        "\n",
        "    return value\n"
      ],
      "metadata": {
        "id": "N6D3Oq1nbVh4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing policy evaluation\n",
        "\n",
        "random_policy = np.ones([noS, 4]) / 4\n",
        "\n",
        "threshold = 0.0001\n",
        "discount = 1.0\n",
        "value = np.zeros((noS,))\n",
        "\n",
        "random_policy_value = policy_evaluation(P, random_policy, threshold, discount)\n",
        "for w_state in wall:\n",
        "    random_policy_value[w_state] = 13\n",
        "print('Value Function for policy:')\n",
        "print(random_policy_value.reshape(grid_size, grid_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPXjB_Hzcxty",
        "outputId": "2238047b-5afa-4d55-ca5c-d76630a7042b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function for policy:\n",
            "[[    0.         -4018.9981729  -6094.93052749 -7365.66336612\n",
            "  -8366.66316978]\n",
            " [-4018.9981729  -4959.06419637 -5897.13035853 -6632.3967717\n",
            "  -7365.66336612]\n",
            " [-6094.93052749 -5897.13035853 -5898.13027261 -5897.13035853\n",
            "  -6094.93052749]\n",
            " [-7365.66336612 -6632.3967717  -5897.13035853 -4959.06419637\n",
            "  -4018.9981729 ]\n",
            " [-8366.66316978 -7365.66336612 -6094.93052749 -4018.9981729\n",
            "      0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Policy Iteration\n",
        "\n",
        "def policy_iteration(P, discount, threshold):\n",
        "    value = np.zeros((noS,))\n",
        "    policy = np.ones([noS, 4]) / 4\n",
        "\n",
        "    while True:\n",
        "        # Policy evaluation\n",
        "        value = policy_evaluation(P, policy, threshold, discount)\n",
        "\n",
        "        new_policy = np.zeros([noS, 4])\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        for s in S:\n",
        "            if not terminal_state(s) and s not in wall:\n",
        "                old_action_probs = policy[s]\n",
        "                action_values = np.zeros(noA)\n",
        "                for a in range(noA):  # Iterating over all the actions\n",
        "                    next_state, probability, reward = P[s][a]\n",
        "                    action_values[a] += probability * (reward + discount * value[next_state])\n",
        "                max_total = np.amax(action_values)  # taking the max reward value\n",
        "                best_a = np.argmax(action_values)\n",
        "\n",
        "                new_policy[s][best_a] = 1\n",
        "\n",
        "                if (np.array_equal(old_action_probs, new_policy[s]) != True):\n",
        "                    policy_stable = False\n",
        "            elif terminal_state(s): # terminal states\n",
        "                new_policy[s][up] = 0.25\n",
        "                new_policy[s][down] = 0.25\n",
        "                new_policy[s][left] = 0.25\n",
        "                new_policy[s][right] = 0.25\n",
        "            elif s in wall: #wall states\n",
        "                new_policy[s][up] = 0.25\n",
        "                new_policy[s][down] = 0.25\n",
        "                new_policy[s][left] = 0.25\n",
        "                new_policy[s][right] = 0.25\n",
        "\n",
        "\n",
        "        if policy_stable:\n",
        "            for w_state in wall:\n",
        "                value[w_state] = 13\n",
        "            return new_policy, value\n",
        "        else:\n",
        "            policy = new_policy\n",
        "\n",
        "# start = time.process_time()\n",
        "best_policy, corr_value = policy_iteration(P, discount, threshold)\n",
        "# end = time.process_time()\n",
        "\n",
        "show_best_policy = np.zeros(noS,)\n",
        "for s, p_s in enumerate(best_policy):\n",
        "    if terminal_state(s):\n",
        "        show_best_policy[s] = 5\n",
        "    elif s in wall:\n",
        "        show_best_policy[s] = 7\n",
        "    else:\n",
        "        show_best_policy[s] = np.argmax(p_s)\n",
        "\n",
        "\n",
        "print('Best policy with Policy Iteration ')\n",
        "print(show_best_policy.reshape(grid_size, grid_size))\n",
        "print('Corresponding Value Function is')\n",
        "print(corr_value.reshape(grid_size, grid_size))\n",
        "print('Time taken')\n",
        "# print(end - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97u8yYTuewzs",
        "outputId": "11287c10-20f2-4dab-b022-a7010b829739"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best policy with Policy Iteration \n",
            "[[5. 2. 2. 2. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 3. 3. 3. 5.]]\n",
            "Corresponding Value Function is\n",
            "[[ 0. -1. -2. -3. -4.]\n",
            " [-1. -2. -3. -4. -3.]\n",
            " [-2. -3. -4. -3. -2.]\n",
            " [-3. -4. -3. -2. -1.]\n",
            " [-4. -3. -2. -1.  0.]]\n",
            "Time taken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###value iteration---> optimal value fuction"
      ],
      "metadata": {
        "id": "sVmJS2tVfEy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(P, discount, threshold):\n",
        "    value = np.zeros((noS,))\n",
        "\n",
        "    while True:\n",
        "        new_policy = np.zeros([noS, 4])\n",
        "        change = 0\n",
        "        for s in S:\n",
        "            if not terminal_state(s) and s not in wall:\n",
        "                v = value[s]\n",
        "                action_values = np.zeros(noA)\n",
        "                for a in range(noA):  # Iterating over all the actions\n",
        "                    next_state, probability, reward = P[s][a]\n",
        "                    action_values[a] += probability * (reward + discount * value[next_state])\n",
        "                max_total = np.amax(action_values)  # max reward value\n",
        "                best_a = np.argmax(action_values)\n",
        "\n",
        "                value[s] = max_total\n",
        "                new_policy[s][best_a] = 1\n",
        "\n",
        "                change = max(change, np.abs(v - value[s]))\n",
        "            elif terminal_state(s):\n",
        "                new_policy[s][up] = 0.25\n",
        "                new_policy[s][down] = 0.25\n",
        "                new_policy[s][left] = 0.25\n",
        "                new_policy[s][right] = 0.25\n",
        "            elif s in wall:\n",
        "                new_policy[s][up] = 0.25\n",
        "                new_policy[s][down] = 0.25\n",
        "                new_policy[s][left] = 0.25\n",
        "                new_policy[s][right] = 0.25\n",
        "\n",
        "\n",
        "        if change < threshold:\n",
        "            break\n",
        "\n",
        "    for w_state in wall:\n",
        "        value[w_state] = 13\n",
        "    return new_policy, value\n",
        "\n",
        "start = time.process_time()\n",
        "best_policy_vi, corr_value_vi = value_iteration(P, discount, threshold)\n",
        "end = time.process_time()\n",
        "\n",
        "show_best_policy_vi = np.zeros(noS,)\n",
        "for s, p_s in enumerate(best_policy_vi):\n",
        "    if terminal_state(s):\n",
        "        show_best_policy_vi[s] = 5\n",
        "    elif s in wall:\n",
        "        show_best_policy_vi[s] = 7\n",
        "    else:\n",
        "        show_best_policy_vi[s] = np.argmax(p_s)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmeoXyhVe2uN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best policy with Value Iteration is')\n",
        "print(show_best_policy_vi.reshape(grid_size, grid_size))\n",
        "print('Corresponding Value Function is')\n",
        "print(corr_value_vi.reshape(grid_size, grid_size))\n",
        "print('Time taken')\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "RXAsPqHFfQMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5685520a-521e-4ca9-853c-ce88127be6a3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best policy with Value Iteration is\n",
            "[[5. 2. 2. 2. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 3. 3. 3. 5.]]\n",
            "Corresponding Value Function is\n",
            "[[ 0. -1. -2. -3. -4.]\n",
            " [-1. -2. -3. -4. -3.]\n",
            " [-2. -3. -4. -3. -2.]\n",
            " [-3. -4. -3. -2. -1.]\n",
            " [-4. -3. -2. -1.  0.]]\n",
            "Time taken\n",
            "0.00267201400000161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdAU8TR3pugR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}