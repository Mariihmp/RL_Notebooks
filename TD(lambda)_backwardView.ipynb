{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfQbqDOP4n7OWrHDJsveGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariihmp/RL_Notebooks/blob/main/TD(lambda)_backwardView.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_folder_path = '/content/drive/MyDrive/session-2'\n",
        "sys.path.append(project_folder_path)"
      ],
      "metadata": {
        "id": "TLIbUA15276U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0602a8ba-c8a4-45ed-e818-b28c37fc647f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GridWorld_env import GridWorld\n",
        "env = GridWorld()\n"
      ],
      "metadata": {
        "id": "bZO95KJaD3Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY_urErD2Q-z"
      },
      "outputs": [],
      "source": [
        "def td_lambda_evaluation(env, policy_to_evaluate, num_episodes, alpha=0.1, lambda_param=0.7):\n",
        "\n",
        "    V = np.zeros(env.num_states)\n",
        "\n",
        "    print(f\"\\nStarting TD(lambda) Policy Evaluation for {num_episodes} episodes\")\n",
        "    print(f\"Alpha={alpha}, Lambda={lambda_param}\")\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        if (i_episode + 1) % (num_episodes // 10 if num_episodes >=10 else 1) == 0:\n",
        "            print(f\"  Episode {i_episode + 1}/{num_episodes}\")\n",
        "\n",
        "        eligibility_traces = np.zeros(env.num_states)\n",
        "        state_idx = env.reset()\n",
        "        done = False\n",
        "        max_steps = 100\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            action_idx = np.random.choice(env.num_actions,\n",
        "                                          p=policy_to_evaluate[state_idx])\n",
        "\n",
        "            next_state_idx, reward, done = env.step(state_idx,\n",
        "                                                    action_idx)\n",
        "\n",
        "            #if terminal, V(next_state_idx) is 0\n",
        "            td_target = reward + (env.gamma * V[next_state_idx] if not done else 0)\n",
        "            td_error = td_target - V[state_idx]\n",
        "\n",
        "            eligibility_traces[state_idx] += 1\n",
        "\n",
        "            eligibility_traces = env.gamma * lambda_param * eligibility_traces\n",
        "            eligibility_traces[state_idx] += 1\n",
        "\n",
        "\n",
        "            eligibility_traces[state_idx] += 1 # Mark presence\n",
        "\n",
        "            # Update V for all states\n",
        "            for s_trace_idx in range(env.num_states):\n",
        "                V[s_trace_idx] += alpha * td_error * eligibility_traces[s_trace_idx]\n",
        "                if env.idx_to_state[s_trace_idx] == env.goal_state:\n",
        "                    V[s_trace_idx] = 0\n",
        "\n",
        "            # Decay traces\n",
        "            eligibility_traces *= env.gamma * lambda_param\n",
        "\n",
        "\n",
        "            state_idx = next_state_idx\n",
        "            steps +=1\n",
        "            if env.idx_to_state[state_idx] == env.goal_state:\n",
        "                V[state_idx] = 0\n",
        "\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's evaluate the optimal policy found by Value Iteration\n",
        "policy_from_vi = optimal_policy_vi # This is already P(a|s)\n",
        "\n",
        "num_td_episodes = 2000\n",
        "alpha_td = 0.1\n",
        "lambda_val = 0.9 # A common choice for lambda\n",
        "\n",
        "print(\"\\n--- Evaluating V(s) for the Policy from Value Iteration using TD(lambda) ---\")\n",
        "V_td_lambda = td_lambda_evaluation(env, policy_from_vi, num_td_episodes, alpha=alpha_td, lambda_param=lambda_val)\n",
        "\n",
        "print(\"\\nV(s) estimated by TD(lambda):\")\n",
        "for s_idx, v_val in enumerate(V_td_lambda):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")\n",
        "\n",
        "print(\"\\nFor comparison, V(s) from Value Iteration:\")\n",
        "for s_idx, v_val in enumerate(V_optimal_vi):\n",
        "    print(f\"  State {env.idx_to_state[s_idx]}: {v_val:.2f}\")"
      ],
      "metadata": {
        "id": "v64Mtw5z20QF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "539e98c0-b07e-4877-b930-61078a1f1753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'optimal_policy_vi' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9f1e726836f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's evaluate the optimal policy found by Value Iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy_from_vi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_policy_vi\u001b[0m \u001b[0;31m# This is already P(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_td_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0malpha_td\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimal_policy_vi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbHiiu2WFsvD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}